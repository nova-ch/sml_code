# # src/scout_ml_package/data/data_manager.py

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from typing import List, Tuple


class HistoricalDataProcessor:
    def __init__(self, task_data_path: str, additional_data_path: str = None):
        """
        Initialize the HistoricalDataProcessor with paths to historical Parquet files.

        Args:
            task_data_path (str): Path to the task data file.
            additional_data_path (str, optional): Path to additional historical data file to merge with.
        """
        self.task_data = pd.read_parquet(task_data_path)
        self.additional_data = (
            pd.read_parquet(additional_data_path)
            if additional_data_path
            else None
        )
        self.merged_data = None

    def filtered_data(self) -> pd.DataFrame:
        """
        Process and filter the data.

        Returns:
            pd.DataFrame: Filtered and processed data.
        """
        if self.additional_data is None:
            return self.task_data

        self.task_data["Process_Head"] = (
            self.task_data["TASKNAME"]
            .str.split(".")
            .str[2]
            .str.replace(r"[_\.]", " ", regex=True)
        )
        self.task_data["Tags"] = (
            self.task_data["TASKNAME"]
            .str.split(".")
            .str[-1]
            .str.replace(r"[_\.]", " ", regex=True)
        )

        self.merged_data = pd.merge(
            self.additional_data,
            self.task_data[["JEDITASKID", "Process_Head", "Tags"]],
            on="JEDITASKID",
            how="left",
        )

        self.merged_data = self.merged_data.drop(
            columns=["PROCESSINGTYPE", "P50", "F50", "PRED_RAM", "TRANSHOME"],
            errors="ignore",
        )
        self.merged_data["IOIntensity"] = self.merged_data[
            "IOINTENSITY"
        ].apply(lambda x: "low" if x < 500 else "high")

        return self.merged_data[
            self.merged_data["PRODSOURCELABEL"].isin(["user", "managed"])
            & (self.merged_data["RAMCOUNT"] > 100)
            & (self.merged_data["RAMCOUNT"] < 6000)
            & (self.merged_data["CPU_EFF"] > 30)
            & (self.merged_data["CPU_EFF"] < 100)
            & (self.merged_data["cputime_HS"] > 0.1)
            & (self.merged_data["cputime_HS"] < 3000)
        ]


class DataSplitter:
    def __init__(
        self, filtered_data: pd.DataFrame, selected_columns: List[str]
    ):
        """
        Initialize the DataSplitter with filtered data and selected columns.

        Args:
            filtered_data (pd.DataFrame): The DataFrame obtained after processing.
            selected_columns (List[str]): List of column names to select from the DataFrame.
        """
        self.merged_data = filtered_data
        self.selected_columns = selected_columns

    def split_data(
        self, test_size: float = 0.30, random_state: int = 42
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split the processed data into training and testing datasets.

        Args:
            test_size (float): Proportion of the dataset to include in the test split.
            random_state (int): Controls the shuffling applied to the data before applying the split.

        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: Training and testing datasets.

        Raises:
            ValueError: If data has not been processed or 'JEDITASKID' is not in selected columns.
        """
        if self.merged_data is None or self.merged_data.empty:
            raise ValueError(
                "Data has not been processed yet. Please provide valid processed data."
            )

        if "JEDITASKID" not in self.selected_columns:
            raise ValueError("The selected columns must include 'JEDITASKID'.")

        df_train, df_test = train_test_split(
            self.merged_data[self.selected_columns].dropna(),
            test_size=test_size,
            random_state=random_state,
        )

        return df_train.reset_index(drop=True), df_test.reset_index(drop=True)


class ModelTrainingInput:
    def __init__(
        self,
        df_train: pd.DataFrame,
        features: List[str],
        target_cols: List[str],
    ):
        """
        Initialize the ModelTrainingInput with training dataset, features, and target columns.

        Args:
            df_train (pd.DataFrame): The training dataset.
            features (List[str]): List of feature column names.
            target_cols (List[str]): List of target column names.
        """
        self.df_train = df_train
        self.features = features
        self.target_cols = target_cols

    def prepare_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Prepare the data for model training.

        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: Feature data and target data for training.
        """
        X_train = self.df_train[self.features]
        y_train = self.df_train[self.target_cols]
        return X_train, y_train


class CategoricalEncoder:
    @staticmethod
    def get_unique_values(df: pd.DataFrame, target_columns: list) -> list:
        """
        Retrieve the unique values for specified columns in a DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame from which unique values are extracted.
            target_columns (list): List of column names to extract unique values from.

        Returns:
            list: A list containing unique values for each specified column.
        """
        return [df[col].unique().tolist() for col in target_columns]

    @staticmethod
    def one_hot_encode(
        df: pd.DataFrame, columns_to_encode: list, category_list: list
    ) -> tuple:
        """
        Perform one-hot encoding on specified categorical columns in a DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame containing the data to be encoded.
            columns_to_encode (list): List of column names to encode.
            category_list (list): List of unique categories for each column.

        Returns:
            tuple: A DataFrame with the original data and the new one-hot encoded features,
                   and a list of new feature names.
        """
        encoder = OneHotEncoder(categories=category_list, sparse_output=False)
        encoded_features = encoder.fit_transform(df[columns_to_encode])
        encoded_feature_names = encoder.get_feature_names_out(
            columns_to_encode
        )
        encoded_df = pd.DataFrame(
            encoded_features, columns=encoded_feature_names, index=df.index
        )
        encoded_df = pd.concat([df, encoded_df], axis=1)
        return encoded_df, encoded_feature_names.tolist()


class BaseDataPreprocessor:
    def __init__(self):
        """Initialize the BaseDataPreprocessor."""
        self.scaler = MinMaxScaler()

    def _fit_and_transform(self, df: pd.DataFrame, numerical_features: list):
        """Fit and transform numerical features."""
        if numerical_features:
            self.scaler.fit(df[numerical_features])
            df[numerical_features] = self.scaler.transform(
                df[numerical_features]
            )
        return df

    def _encode_features(
        self, df: pd.DataFrame, categorical_features: list, category_list: list
    ):
        """Encode categorical features."""
        return CategoricalEncoder.one_hot_encode(
            df, categorical_features, category_list
        )


class TrainingDataPreprocessor(BaseDataPreprocessor):
    def preprocess(
        self,
        df: pd.DataFrame,
        numerical_features: list,
        categorical_features: list,
        category_list: list,
    ) -> tuple:
        """
        Preprocess training data.

        Args:
            df (pd.DataFrame): The training data.
            numerical_features (list): List of numerical feature names.
            categorical_features (list): List of categorical feature names.
            category_list (list): List of unique categories for each categorical feature.

        Returns:
            tuple: Preprocessed DataFrame, encoded column names, and fitted scaler.
        """
        df = self._fit_and_transform(df, numerical_features)
        df, encoded_columns = self._encode_features(
            df, categorical_features, category_list
        )
        return df, encoded_columns, self.scaler


class NewDataPreprocessor(BaseDataPreprocessor):
    def preprocess(
        self,
        new_data: pd.DataFrame,
        numerical_features: list,
        categorical_features: list,
        category_list: list,
        scaler: MinMaxScaler,
        encoded_columns: list,
    ) -> pd.DataFrame:
        """
        Preprocess new data for predictions using the fitted scaler from training data.

        Args:
            new_data (pd.DataFrame): The new data to preprocess.
            numerical_features (list): List of numerical feature names.
            categorical_features (list): List of categorical feature names.
            category_list (list): List of unique categories for each categorical feature.
            scaler (MinMaxScaler): Fitted scaler from training data.
            encoded_columns (list): List of encoded column names.

        Returns:
            pd.DataFrame: Preprocessed new data.
        """
        new_data[numerical_features] = scaler.transform(
            new_data[numerical_features]
        )
        new_data, _ = self._encode_features(
            new_data, categorical_features, category_list
        )
        for col in encoded_columns:
            if col not in new_data.columns:
                new_data[col] = 0
        return new_data[encoded_columns + numerical_features]


class LiveDataPreprocessor(BaseDataPreprocessor):
    def preprocess(
        self,
        live_data: pd.DataFrame,
        numerical_features: list,
        categorical_features: list,
        category_list: list,
        scaler: MinMaxScaler,
    ) -> Tuple[pd.DataFrame, List[str]]:
        """
        Preprocess live data for predictions using the fitted scaler from training data.

        Args:
            live_data (pd.DataFrame): The live data to preprocess.
            numerical_features (list): List of numerical feature names.
            categorical_features (list): List of categorical feature names.
            category_list (list): List of unique categories for each categorical feature.
            scaler (MinMaxScaler): Fitted scaler from training data.

        Returns:
            Tuple[pd.DataFrame, List[str]]: Preprocessed live data and list of encoded column names.
        """
        live_data = live_data.copy()
        live_data[numerical_features] = scaler.transform(
            live_data[numerical_features]
        )
        live_data, encoded_columns = self._encode_features(
            live_data, categorical_features, category_list
        )
        return live_data[encoded_columns + numerical_features], encoded_columns
